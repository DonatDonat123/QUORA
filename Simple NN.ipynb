{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time, sys, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "# For Tensorflow & Keras\n",
    "import tensorflow as tf\n",
    "import keras.layers as lyr\n",
    "from keras.models import Model\n",
    "#For Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#For XGBoost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT DATA\n",
      "LOAD TRAIN DATA\n",
      "LOAD TEST DATA\n",
      "FIT CORPUS ON COUNT VECTORIZER\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "X_train, X_traintest, Y_train, Y_traintest = splitData()\n",
    "X_test, test_idxs = testData()\n",
    "\n",
    "# 1st Stage, Question to Int Array\n",
    "# Build Vocabulary\n",
    "\n",
    "f1 = BoW()\n",
    "f1.fit(X_train.append(X_traintest))\n",
    "X_all = X_train.append(X_traintest)\n",
    "mycv = f1.getVectorizer()\n",
    "#words to tokens\n",
    "X_test_q1 = create_padded_seqs(X_test.question1,mycv)\n",
    "X_test_q2 = create_padded_seqs(X_test.question2, mycv)\n",
    "X_train_q1 = create_padded_seqs(X_train.question1, mycv)\n",
    "X_train_q2 = create_padded_seqs(X_train.question2, mycv)\n",
    "X_traintest_q1 = create_padded_seqs(X_traintest.question1, mycv)\n",
    "X_traintest_q2 = create_padded_seqs(X_traintest.question2, mycv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mycv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)            (None, 10)            0           input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             11          multiply_2[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Feed NN with Padded Sequences\n",
    "\n",
    "input1_tensor = lyr.Input(X_train_q1.shape[1:])\n",
    "input2_tensor = lyr.Input(X_train_q2.shape[1:])\n",
    "\n",
    "#words_embedding_layer = lyr.Embedding(X1_train_q1.max() + 1, 100)\n",
    "merge_layer = lyr.multiply([input1_tensor, input2_tensor])\n",
    "ouput_layer = lyr.Dense(1, activation='sigmoid')(merge_layer)\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283001 samples, validate on 121287 samples\n",
      "Epoch 1/6\n",
      "2s - loss: 9.7523 - val_loss: 9.7517\n",
      "Epoch 2/6\n",
      "2s - loss: 9.7523 - val_loss: 9.7517\n",
      "Epoch 3/6\n",
      "2s - loss: 9.7523 - val_loss: 9.7517\n",
      "Epoch 4/6\n",
      "2s - loss: 9.7523 - val_loss: 9.7517\n",
      "Epoch 5/6\n",
      "2s - loss: 9.7523 - val_loss: 9.7517\n",
      "Epoch 6/6\n",
      "2s - loss: 9.7523 - val_loss: 9.7517\n",
      "[0]\ttrain-logloss:0.682886\tval-logloss:0.683174\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[10]\ttrain-logloss:0.633481\tval-logloss:0.63598\n",
      "[20]\ttrain-logloss:0.611799\tval-logloss:0.617023\n",
      "[30]\ttrain-logloss:0.597875\tval-logloss:0.605675\n",
      "[40]\ttrain-logloss:0.589812\tval-logloss:0.599589\n",
      "[50]\ttrain-logloss:0.580111\tval-logloss:0.592039\n",
      "[60]\ttrain-logloss:0.573041\tval-logloss:0.586838\n",
      "[70]\ttrain-logloss:0.567315\tval-logloss:0.582681\n",
      "[80]\ttrain-logloss:0.561625\tval-logloss:0.578658\n",
      "[90]\ttrain-logloss:0.557021\tval-logloss:0.575543\n",
      "[100]\ttrain-logloss:0.552771\tval-logloss:0.572582\n",
      "[110]\ttrain-logloss:0.54788\tval-logloss:0.569264\n",
      "[120]\ttrain-logloss:0.543733\tval-logloss:0.566547\n",
      "[130]\ttrain-logloss:0.540531\tval-logloss:0.564611\n",
      "[140]\ttrain-logloss:0.536836\tval-logloss:0.56229\n",
      "[150]\ttrain-logloss:0.533209\tval-logloss:0.559846\n",
      "[160]\ttrain-logloss:0.53058\tval-logloss:0.558275\n",
      "[170]\ttrain-logloss:0.527801\tval-logloss:0.556708\n",
      "[180]\ttrain-logloss:0.524503\tval-logloss:0.554683\n",
      "[190]\ttrain-logloss:0.521273\tval-logloss:0.552786\n",
      "[200]\ttrain-logloss:0.518928\tval-logloss:0.551479\n",
      "[210]\ttrain-logloss:0.516344\tval-logloss:0.550117\n",
      "[220]\ttrain-logloss:0.514095\tval-logloss:0.548837\n",
      "[230]\ttrain-logloss:0.511828\tval-logloss:0.547623\n",
      "[240]\ttrain-logloss:0.51\tval-logloss:0.546643\n",
      "[250]\ttrain-logloss:0.508085\tval-logloss:0.545559\n",
      "[260]\ttrain-logloss:0.505874\tval-logloss:0.54436\n",
      "[270]\ttrain-logloss:0.503727\tval-logloss:0.543245\n",
      "[280]\ttrain-logloss:0.50128\tval-logloss:0.541923\n",
      "[290]\ttrain-logloss:0.499564\tval-logloss:0.54106\n",
      "[300]\ttrain-logloss:0.497629\tval-logloss:0.540099\n",
      "[310]\ttrain-logloss:0.496018\tval-logloss:0.5393\n",
      "[320]\ttrain-logloss:0.494092\tval-logloss:0.538301\n",
      "[330]\ttrain-logloss:0.492477\tval-logloss:0.537502\n",
      "[340]\ttrain-logloss:0.490485\tval-logloss:0.53652\n",
      "[350]\ttrain-logloss:0.489166\tval-logloss:0.535927\n",
      "[360]\ttrain-logloss:0.487482\tval-logloss:0.53513\n",
      "[370]\ttrain-logloss:0.485766\tval-logloss:0.534279\n",
      "[380]\ttrain-logloss:0.484302\tval-logloss:0.533523\n",
      "[390]\ttrain-logloss:0.482615\tval-logloss:0.532764\n",
      "[400]\ttrain-logloss:0.481183\tval-logloss:0.532115\n",
      "[410]\ttrain-logloss:0.479593\tval-logloss:0.53141\n",
      "[420]\ttrain-logloss:0.478403\tval-logloss:0.530877\n",
      "[430]\ttrain-logloss:0.476943\tval-logloss:0.530221\n",
      "[440]\ttrain-logloss:0.475477\tval-logloss:0.529575\n",
      "[450]\ttrain-logloss:0.473982\tval-logloss:0.528899\n",
      "[460]\ttrain-logloss:0.472865\tval-logloss:0.528357\n",
      "[470]\ttrain-logloss:0.471238\tval-logloss:0.527637\n",
      "[480]\ttrain-logloss:0.469937\tval-logloss:0.527056\n",
      "[490]\ttrain-logloss:0.468575\tval-logloss:0.526528\n",
      "[500]\ttrain-logloss:0.467422\tval-logloss:0.525995\n",
      "[510]\ttrain-logloss:0.466254\tval-logloss:0.525455\n",
      "[520]\ttrain-logloss:0.465297\tval-logloss:0.525037\n",
      "[530]\ttrain-logloss:0.463972\tval-logloss:0.5245\n",
      "[540]\ttrain-logloss:0.462689\tval-logloss:0.524012\n",
      "[550]\ttrain-logloss:0.461406\tval-logloss:0.523513\n",
      "[560]\ttrain-logloss:0.460323\tval-logloss:0.523077\n",
      "[570]\ttrain-logloss:0.45922\tval-logloss:0.522578\n",
      "[580]\ttrain-logloss:0.457841\tval-logloss:0.522023\n",
      "[590]\ttrain-logloss:0.456534\tval-logloss:0.521544\n",
      "[600]\ttrain-logloss:0.455377\tval-logloss:0.521075\n",
      "[610]\ttrain-logloss:0.454386\tval-logloss:0.520737\n",
      "[620]\ttrain-logloss:0.453178\tval-logloss:0.52028\n",
      "[630]\ttrain-logloss:0.452095\tval-logloss:0.519902\n",
      "[640]\ttrain-logloss:0.451148\tval-logloss:0.519567\n",
      "[650]\ttrain-logloss:0.450075\tval-logloss:0.519168\n",
      "[660]\ttrain-logloss:0.44894\tval-logloss:0.518713\n",
      "[670]\ttrain-logloss:0.447931\tval-logloss:0.518369\n",
      "[680]\ttrain-logloss:0.446725\tval-logloss:0.51785\n",
      "[690]\ttrain-logloss:0.44577\tval-logloss:0.517505\n",
      "[700]\ttrain-logloss:0.444728\tval-logloss:0.517144\n",
      "[710]\ttrain-logloss:0.443541\tval-logloss:0.516679\n",
      "[720]\ttrain-logloss:0.442393\tval-logloss:0.516212\n",
      "[730]\ttrain-logloss:0.441537\tval-logloss:0.515905\n",
      "[740]\ttrain-logloss:0.440457\tval-logloss:0.515479\n",
      "[750]\ttrain-logloss:0.439565\tval-logloss:0.515173\n",
      "[760]\ttrain-logloss:0.438596\tval-logloss:0.514805\n",
      "[770]\ttrain-logloss:0.437671\tval-logloss:0.514503\n",
      "[780]\ttrain-logloss:0.436712\tval-logloss:0.51418\n",
      "[790]\ttrain-logloss:0.435856\tval-logloss:0.513858\n",
      "[800]\ttrain-logloss:0.434855\tval-logloss:0.51349\n",
      "[810]\ttrain-logloss:0.433929\tval-logloss:0.513208\n",
      "[820]\ttrain-logloss:0.432988\tval-logloss:0.512837\n",
      "[830]\ttrain-logloss:0.43211\tval-logloss:0.512587\n",
      "[840]\ttrain-logloss:0.431377\tval-logloss:0.512333\n",
      "[850]\ttrain-logloss:0.430537\tval-logloss:0.512057\n",
      "[860]\ttrain-logloss:0.429755\tval-logloss:0.511851\n",
      "[870]\ttrain-logloss:0.428806\tval-logloss:0.511496\n",
      "[880]\ttrain-logloss:0.427883\tval-logloss:0.511196\n",
      "[890]\ttrain-logloss:0.427077\tval-logloss:0.510935\n",
      "[900]\ttrain-logloss:0.426384\tval-logloss:0.510713\n",
      "[910]\ttrain-logloss:0.425466\tval-logloss:0.510372\n",
      "[920]\ttrain-logloss:0.424551\tval-logloss:0.510058\n",
      "[930]\ttrain-logloss:0.423779\tval-logloss:0.509811\n",
      "[940]\ttrain-logloss:0.422962\tval-logloss:0.509535\n",
      "[950]\ttrain-logloss:0.422297\tval-logloss:0.509334\n",
      "[960]\ttrain-logloss:0.421446\tval-logloss:0.509071\n",
      "[970]\ttrain-logloss:0.420637\tval-logloss:0.508799\n",
      "[980]\ttrain-logloss:0.419894\tval-logloss:0.508567\n",
      "[990]\ttrain-logloss:0.41902\tval-logloss:0.50827\n"
     ]
    }
   ],
   "source": [
    "# FIT --> Train NN weights\n",
    "model.fit([X_train_q1, X_train_q2], Y_train, \n",
    "          validation_data=([X_traintest_q1, X_traintest_q2], Y_traintest), \n",
    "          batch_size=128, epochs=6, verbose=2)\n",
    "\n",
    "# (1) Take Features from Merge Layer and(2) feed to Classifier (XGBoost)\n",
    "#(1)\n",
    "mergeNN = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "mergeNN.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "F_train = mergeNN.predict([X_train_q1, X_train_q1], batch_size=128)\n",
    "F_traintest = mergeNN.predict([X_traintest_q1, X_traintest_q2], batch_size=128)\n",
    "F_test = mergeNN.predict([X_test_q1, X_test_q2], batch_size=128)\n",
    "\n",
    "#(2)\n",
    "dTrain = xgb.DMatrix(F_train, label=Y_train)\n",
    "dTraintest = xgb.DMatrix(F_traintest, label=Y_traintest)\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n",
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dTraintest,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.507999677833\n",
      "Create Submission File\n"
     ]
    }
   ],
   "source": [
    "# Predict Validation Set and Compute Log Loss \n",
    "X_val = xgb.DMatrix(F_traintest)\n",
    "Y_pred = bst.predict(X_val, ntree_limit=bst.best_ntree_limit)\n",
    "Y_true = Y_traintest\n",
    "print log_loss(Y_true, Y_pred)\n",
    "# PREDICT Submission File\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "df_sub = pd.DataFrame({\n",
    "        'test_id': test_idxs,\n",
    "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')\n",
    "\n",
    "\n",
    "print(\"Create Submission File\")\n",
    "df_sub.to_csv('newsub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345790"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_padded_seqs(texts, mycv):\n",
    "    seqs = texts.apply(text2ints, cv=mycv)\n",
    "    return pad_sequences(seqs, maxlen=10)\n",
    "\n",
    "def text2ints(text, cv):\n",
    "    other_index = len(cv.vocabulary_)\n",
    "    intseq = []\n",
    "    mysplit = text.split(\" \")\n",
    "    filter(lambda x: x!=\"\", mysplit)\n",
    "    for word in mysplit:\n",
    "        intseq.append(cv.vocabulary_[word]) if word in cv.vocabulary_ else other_index\n",
    "    return intseq\n",
    "\n",
    "#Train_Path = './input/df10.csv' # JUST TO TEST FASTER , normally train.csv\n",
    "#Test_Path = './input/df10.csv'\n",
    "Train_Path = './input/train.csv'\n",
    "Test_Path = './input/test.csv'\n",
    "\n",
    "def cleanreview(review):\n",
    "    letters_only = re.sub(\"[^a-z]\", \" \", review.lower())\n",
    "    return letters_only\n",
    "def cleanquestions(df):\n",
    "\n",
    "    question1 = df.question1.apply(lambda x: cleanreview(x))\n",
    "    question2 = df.question2.apply(lambda x: cleanreview(x))\n",
    "    df.loc[:, 'question1'] = pd.Series(question1, index=df.index)\n",
    "    df.loc[:, 'question2'] = pd.Series(question1, index=df.index)\n",
    "    return df\n",
    "\n",
    "def load_train():\n",
    "    print(\"LOAD TRAIN DATA\")\n",
    "    train = pd.read_csv(Train_Path).dropna()\n",
    "    train = cleanquestions(train)\n",
    "    train['question1'].fillna('', inplace=True)\n",
    "    train['question2'].fillna('', inplace=True)\n",
    "\n",
    "    return train\n",
    "def load_test():\n",
    "    print(\"LOAD TEST DATA\")\n",
    "    test = pd.read_csv(Test_Path).dropna()\n",
    "    test = cleanquestions(test)\n",
    "    test['question1'].fillna('', inplace=True)\n",
    "    test['question2'].fillna('', inplace=True)\n",
    "\n",
    "    return test\n",
    "def splitData(ratio = 0.7): # ratio = train_set, between 0-1.0, default 0.7\n",
    "    print(\"SPLIT DATA\")\n",
    "    #time.sleep(1)\n",
    "    train = load_train()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train.loc[:,[\"question1\", \"question2\"]],\\\n",
    "                                                        train[\"is_duplicate\"],\\\n",
    "                                                        train_size=0.7, random_state=42)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "def trainData():\n",
    "    print(\"LOAD TRAIN DATA FOR PREDICT AND SUBMIT\")\n",
    "    train = load_train()\n",
    "    X = train.loc[:, [\"question1\", \"question2\"]]\n",
    "    Y = train.loc[:][\"is_duplicate\"]\n",
    "    return X,Y\n",
    "def testData():\n",
    "    test = load_test()\n",
    "    X_test = test.loc[:,[\"question1\", \"question2\"]]\n",
    "    X_ids = test.loc[:][\"test_id\"]\n",
    "    return X_test, X_ids\n",
    "\n",
    "def rprint(str): # Next print overwrites this, i.e use for indicate progress\n",
    "\tsys.stdout.write(\"PROCESSING: \" + str + \"            \\r\")\n",
    "\tsys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoW:\n",
    "    name = \"BoW\"\n",
    "    cv = 0\n",
    "\n",
    "    def fit(self, X):\n",
    "        corpus = pd.concat([X.question1, X.question2])\n",
    "        self.cv = CountVectorizer(analyzer='word',min_df = 0,\n",
    "        max_features=5000, ngram_range=(1,1), preprocessor=None, stop_words=None,\n",
    "        tokenizer=None)\n",
    "        print(\"FIT CORPUS ON COUNT VECTORIZER\")\n",
    "        self.cv.fit(corpus)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.cv.transform(X)\n",
    "    def getVectorizer(self):\n",
    "        return self.cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
