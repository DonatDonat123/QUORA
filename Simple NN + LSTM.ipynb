{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time, sys, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "# For Tensorflow & Keras\n",
    "import tensorflow as tf\n",
    "import keras.layers as lyr\n",
    "from keras.models import Model\n",
    "#For Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#For XGBoost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stopwords\n",
      "SPLIT DATA\n",
      "LOAD TRAIN DATA\n",
      "LOAD TEST DATA\n",
      "FIT CORPUS ON COUNT VECTORIZER\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE HELPER FILES AND BOW first\n",
    "# DEFINE GLOBAL VALUES\n",
    "STOPS = prep_stops()\n",
    "#Train_Path = './input/df10.csv' # JUST TO TEST FASTER , normally train.csv\n",
    "#Test_Path = './input/test1000.csv'\n",
    "lstm_units = 256 # Lower if network takes too long\n",
    "Train_Path = './input/train.csv'\n",
    "Test_Path = './input/test.csv'\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "X_train, X_traintest, Y_train, Y_traintest = splitData()\n",
    "X_test, test_idxs = testData()\n",
    "\n",
    "# 1st Stage, Question to Int Array\n",
    "# Build Vocabulary\n",
    "\n",
    "f1 = BoW()\n",
    "f1.fit(X_train.append(X_traintest))\n",
    "X_all = X_train.append(X_traintest)\n",
    "mycv = f1.getVectorizer()\n",
    "#words to tokens\n",
    "X_test_q1 = create_padded_seqs(X_test.question1,mycv)\n",
    "X_test_q2 = create_padded_seqs(X_test.question2, mycv)\n",
    "X_train_q1 = create_padded_seqs(X_train.question1, mycv)\n",
    "X_train_q2 = create_padded_seqs(X_train.question2, mycv)\n",
    "X_traintest_q1 = create_padded_seqs(X_traintest.question1, mycv)\n",
    "X_traintest_q2 = create_padded_seqs(X_traintest.question2, mycv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_33 (InputLayer)            (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_34 (InputLayer)            (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)         (None, 10, 5001)      25010001    input_33[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)         (None, 10, 5001)      25010001    input_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                   (None, 64)            1296896     embedding_17[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                   (None, 64)            1296896     embedding_18[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)           (None, 64)            0           lstm_23[0][0]                    \n",
      "                                                                   lstm_24[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             65          multiply_11[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 52,613,859\n",
      "Trainable params: 52,613,859\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Feed NN with Padded Sequences\n",
    "\n",
    "input1_tensor = lyr.Input(X_train_q1.shape[1:])\n",
    "input2_tensor = lyr.Input(X_train_q2.shape[1:])\n",
    "e1 = lyr.Embedding(output_dim=5001, input_dim=5001)(input1_tensor) # Input Dimension = max_features of CountVec\n",
    "e2 = lyr.Embedding(output_dim=5001, input_dim=5001)(input2_tensor)\n",
    "lstm1 = lyr.LSTM(lstm_units, activation='tanh')(e1)\n",
    "lstm2 = lyr.LSTM(lstm_units, activation='tanh')(e2)\n",
    "# Feed input1 and input2 seperately to LSTM:\n",
    "merge_layer = lyr.multiply([lstm1, lstm2])  ### OLD MERGE LAYER\n",
    "ouput_layer = lyr.Dense(1, activation='sigmoid')(merge_layer)\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD NN\n",
    "# Feed NN with Padded Sequences\n",
    "\n",
    "#input1_tensor = lyr.Input(X_train_q1.shape[1:])\n",
    "#input2_tensor = lyr.Input(X_train_q2.shape[1:])\n",
    "\n",
    "#merge_layer = lyr.multiply([input1_tensor, input2_tensor])  ### OLD MERGE LAYER\n",
    "#ouput_layer = lyr.Dense(1, activation='sigmoid')(merge_layer)\n",
    "#model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/6\n",
      "82s - loss: 0.6538 - val_loss: 0.6043\n",
      "Epoch 2/6\n",
      "94s - loss: 0.4886 - val_loss: 0.6424\n",
      "Epoch 3/6\n",
      "94s - loss: 0.3271 - val_loss: 0.8185\n",
      "Epoch 4/6\n",
      "94s - loss: 0.2150 - val_loss: 1.0052\n",
      "Epoch 5/6\n",
      "93s - loss: 0.1510 - val_loss: 1.2001\n",
      "Epoch 6/6\n",
      "94s - loss: 0.1049 - val_loss: 1.3517\n",
      "[0]\ttrain-logloss:0.60797\tval-logloss:0.669502\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[10]\ttrain-logloss:0.220831\tval-logloss:0.686722\n",
      "Stopping. Best iteration:\n",
      "[4]\ttrain-logloss:0.385155\tval-logloss:0.638979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FIT --> Train NN weights\n",
    "model.fit([X_train_q1, X_train_q2], Y_train, \n",
    "          validation_data=([X_traintest_q1, X_traintest_q2], Y_traintest), \n",
    "          batch_size=128, epochs=6, verbose=2)\n",
    "\n",
    "# (1) Take Features from Merge Layer and(2) feed to Classifier (XGBoost)\n",
    "#(1)\n",
    "mergeNN = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "mergeNN.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "F_train = mergeNN.predict([X_train_q1, X_train_q1], batch_size=128)\n",
    "F_traintest = mergeNN.predict([X_traintest_q1, X_traintest_q2], batch_size=128)\n",
    "F_test = mergeNN.predict([X_test_q1, X_test_q2], batch_size=128)\n",
    "\n",
    "#(2)\n",
    "dTrain = xgb.DMatrix(F_train, label=Y_train)\n",
    "dTraintest = xgb.DMatrix(F_traintest, label=Y_traintest)\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n",
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dTraintest,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Submission File\n"
     ]
    }
   ],
   "source": [
    "# PREDICT Submission File\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "df_sub = pd.DataFrame({\n",
    "        'test_id': test_idxs,\n",
    "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')\n",
    "\n",
    "\n",
    "print(\"Create Submission File\")\n",
    "df_sub.to_csv('newsub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute LogLoss in Validation Set --> Same as in XG-Boost output\n",
    "#X_val = xgb.DMatrix(F_traintest)\n",
    "#Y_pred = bst.predict(X_val, ntree_limit=bst.best_ntree_limit)\n",
    "#Y_true = Y_traintest\n",
    "#print log_loss(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stopwords\n"
     ]
    }
   ],
   "source": [
    "def prep_stops():\n",
    "\tprint \"Creating stopwords\"\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\torig_stops = set(stopwords.words(\"english\"))\n",
    "\tnot_stops = set([\"what\", \"how\", \"who\"]) # add more if you like\n",
    "\tstops = orig_stops - not_stops\t\t# Set difference\n",
    "\treturn stops\n",
    "\n",
    "def create_padded_seqs(texts, mycv):\n",
    "    seqs = texts.apply(text2ints, cv=mycv)\n",
    "    return pad_sequences(seqs, maxlen=10)\n",
    "\n",
    "def text2ints(text, cv):\n",
    "    other_index = len(cv.vocabulary_)\n",
    "    intseq = []\n",
    "    mysplit = text.split(\" \")\n",
    "    filter(lambda x: x!=\"\", mysplit)\n",
    "    for word in mysplit:\n",
    "        intseq.append(cv.vocabulary_[word]) if word in cv.vocabulary_ else other_index\n",
    "    return intseq\n",
    "\n",
    "def cleanreview(review):\n",
    "    review = re.sub(\"[^a-z]\", \" \", review.lower()) # letters only\n",
    "    review = review.split(\" \")\n",
    "    meaningful_words = [w for w in review if not w in STOPS]\n",
    "    sentence = \" \".join(meaningful_words)\n",
    "    return sentence\n",
    "    \n",
    "def cleanquestions(df):\n",
    "\n",
    "    question1 = df.question1.apply(lambda x: cleanreview(x))\n",
    "    question2 = df.question2.apply(lambda x: cleanreview(x))\n",
    "    df.loc[:, 'question1'] = pd.Series(question1, index=df.index)\n",
    "    df.loc[:, 'question2'] = pd.Series(question1, index=df.index)\n",
    "    return df\n",
    "\n",
    "def load_train():\n",
    "    print(\"LOAD TRAIN DATA\")\n",
    "    train = pd.read_csv(Train_Path)\n",
    "    train['question1'].fillna('', inplace=True)\n",
    "    train['question2'].fillna('', inplace=True)\n",
    "    train = cleanquestions(train)\n",
    "    return train\n",
    "def load_test():\n",
    "    print(\"LOAD TEST DATA\")\n",
    "    test = pd.read_csv(Test_Path)\n",
    "    test['question1'].fillna('', inplace=True)\n",
    "    test['question2'].fillna('', inplace=True)\n",
    "    test = cleanquestions(test)\n",
    "    return test\n",
    "def splitData(ratio = 0.7): # ratio = train_set, between 0-1.0, default 0.7\n",
    "    print(\"SPLIT DATA\")\n",
    "    #time.sleep(1)\n",
    "    train = load_train()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train.loc[:,[\"question1\", \"question2\"]],\\\n",
    "                                                        train[\"is_duplicate\"],\\\n",
    "                                                        train_size=0.7, random_state=42)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "def trainData():\n",
    "    print(\"LOAD TRAIN DATA FOR PREDICT AND SUBMIT\")\n",
    "    train = load_train()\n",
    "    X = train.loc[:, [\"question1\", \"question2\"]]\n",
    "    Y = train.loc[:][\"is_duplicate\"]\n",
    "    return X,Y\n",
    "def testData():\n",
    "    test = load_test()\n",
    "    X_test = test.loc[:,[\"question1\", \"question2\"]]\n",
    "    X_ids = test.loc[:][\"test_id\"]\n",
    "    return X_test, X_ids\n",
    "\n",
    "def rprint(str): # Next print overwrites this, i.e use for indicate progress\n",
    "\tsys.stdout.write(\"PROCESSING: \" + str + \"            \\r\")\n",
    "\tsys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoW:\n",
    "    name = \"BoW\"\n",
    "    cv = 0\n",
    "\n",
    "    def fit(self, X):\n",
    "        corpus = pd.concat([X.question1, X.question2])\n",
    "        self.cv = CountVectorizer(analyzer='word',min_df = 0,\n",
    "        max_features=5000, ngram_range=(1,1), preprocessor=None, stop_words=None,\n",
    "        tokenizer=None)\n",
    "        print(\"FIT CORPUS ON COUNT VECTORIZER\")\n",
    "        self.cv.fit(corpus)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.cv.transform(X)\n",
    "    def getVectorizer(self):\n",
    "        return self.cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
